# üìñ Critica: Bayesian personalized ranking from implicit feedback...

### üìöContexto:
Entre los problemas de recomendaci√≥n (personalizada), se entiende por _item-recommendation_ al problema en el cual se busca entregar una lista personalizada y ordenada de _items_ (**ranking**) para cada usuario. La idea es estudiar las interecciones pasadas que cada usuario a tenido y, a partir de estas, identificar por cuales _items_ el usuario puede tener mayor preferencia. Estas interacciones _item-usuario_ pueden ser de forma expl√≠cita (_rating_, _like_, _dislike_, etc) o impl√≠cita (tiempo, fecha, clicks) y, desde estas, diferentes modelos se han desarrollado para aprovechar uno o los dos tipos de informaci√≥n.

Este paper explora una alternativa al _approach_ usual que han tenido las t√©cnicas centradas en _implicit data_, bajo la premisa de que la mayor√≠a de estos modelos son entrenados con ciertas m√©tricas (**preferencia individual**), pero evaluados bajo tareas distinta. Con esta idea se propone un esquema de optimizaci√≥n y entrenamiento que es capaz de reducir el tiempo de entrenamiento y mejorar ampliamente el rendimiento sobre los modelos en los que se aplican.

### üßæ Propuesta:
Uno de los principales problemas a la hora de construir recomendadores con informaci√≥n impl√≠cita, es que estos _datasets_ carecen de una distinci√≥n clara entre **_feeback_ negativo** y datos "nulos" (ie: que el usuario piense comprar en el futuro o no haya "encontrado" el item). Para solucionar este problema, los autores proponen descomponer los datos de training en tipletas (u, i, j), donde se denota **solo la preferencia de i sobre j**, de esta forma podemos utilizar _feeback_ positivo y dejar vac√≠o las tuplas donde no se puede asegurar preferencia. A partir de esta organizaci√≥n de los datos, los autores presentan su criterio de optimizaci√≥n **BPR-Opt**:

#### üéñ Esquema de entrenamiento:
Basado en an√°lisis bayesiano, se busca maximizar la probabilidad posterior del conjunto de preferencias del usuario _u_ y, desde su expresi√≥n por cada _item_, se desprende la siguiente funci√≥n de p√©rdida:

> <img src="images/125e17548ee7bac25e64b3886c3b85b0bcabb715bb06119e7c5dde4926327d36.png" width="400" height=300>

En la cual se utiliza **&sigma;(x)** que denota la funci√≥n sigmoidea y lo hace que toda la expresi√≥n sea diferenciable. Ac√°, debo destacar que en el paper se presenta un (muy) extenso y riguroso an√°lisis estad√≠sticio del porqu√© esta formulaci√≥n funciona y c√≥mo asegura llegar al resultado deseado (lo cual es una aspecto positivo, pero hizo un poco dif√≠cil la lectura).

Finalmente, utilizando la funci√≥n de p√©rdida se formula un esquema de entrenamiento llamado **LearnBPR**, el cual es un algoritmo de _stochastic gradient descent_, pero que introduce aleatoreidad (con reemplazo) a la hora de elegir el siguiente dato con el cual entrenar.

### üíª Aspectos Positivos:

Creo que es importante **destacar el primer motivo del paper**: que las m√©tricas de evaluaci√≥n y entrenamiento pueden no ser siempre las correctas para los que se busca aprender. Esto es algo que discut√≠ en los comentarios anteriores y creo que es clave a la hora de explicar los buenos resultados de este esquema.

Por la parte de experimentaci√≥n, los resultados hablan por si solos, el m√©todo propuesto no solo converge mucho m√°s r√°pido que los m√©todos tradicionales, sino que tambi√©n mejora el rendimiento en gran medida. Adem√°s, creo que la *8experimentaci√≥n que realizaron fue bastante exhaustiva**, en el sentido que probaron y reportaron los resultado de su esquema aplicado a varios modelos bien populares.

Finalmente, el hecho de haber hecho su an√°lisis sobre dos _datasets_ diferentes (y de naturaleza distinta), aporta a dar m√°s una validez general del esquema. Ac√° quiz√°s se pudiera incluir un an√°lisis en _datasets_ donde el _ratio_ ```users/items``` sea menor a 1, pero creo que no era el principal objetivo del paper.

### üìï Aspectos en contra:
Realmente, el √∫nico punto que no se abarc√≥ fue el m√©todo de _Maximum Margin Matrix Factorization (MMMF)_ , del cual los autores hablan y comparan con el suyo, pero no llegan a compararlo. Creo que el principal problema con esto es que se habla de que son m√©todos muy similares e incluso se propone un esquema para que el algoritmo funcione con informaci√≥n √≠mplicita, pero a√∫n as√≠ no se hace experimentalmente. Esto me da algo de inseguridad respecto a si este m√©todo puede tener mejores resultados que el que ellos propusieron.

### üéâ Una conclusi√≥n:
Me parece que este paper cumple muy bien su objetivo al proponer este gran esquema de optimizaci√≥n y entrenamiento. Especialmente, me parece muy importante la idea que tuvieron para manejar los datos datos nulos vs el _feedback_ negativo, pues si bien no es el ideal, creo que es capaz de utilizar m√°s informaci√≥n que los m√©todos que exist√≠an antes de este paper. Por otra parte, la idea de cambiar **lo que se est√° optimizando** creo que fue un importante paso hacia obtener mejores m√©tricas de evaluaci√≥n/entrenamiento.
