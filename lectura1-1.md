# üìñ Critica: Item-based collaborative filtering recommendation algorithms...

### üìöContexto:
Los algoritmos que utilizan _Collaborative Filtering (CF)_ son m√©todos de recomendaci√≥n que se basan en buscar similitudes entres usuarios o _items_ y, a partir de estas, generar recomendaciones. Estos algoritmos se pueden dividir en _memory-based methods_ y _model-base methods_, entre los primeros se encuentran los _user-based_ e _item-based_ (seg√∫n [2]) que ser√°n los principales puntos de discusi√≥n en el art√≠culo.

Si bien algoritmos que emplean _CF_ y, espec√≠ficamente, _user-based CF_ son los algoritmos m√°s populares y utilizados en la pr√°ctica (en los a√±os del paper), la expansi√≥n de las aplicaciones _web_ y los millones de nuevos datos que estas generan ha revelado problemas espec√≠ficos en estos cuando se habla de **escalabilidad** o de **vac√≠os en los datos** (_sparsity_).


### üßæ Item-Based CF:
Ante estas problem√°ticas, los autores proponen un esquema _item-based CF_ que ataca ambos problemas y que, seg√∫n los experimentos realizados, resulta en un menor tiempo de computo y mejores rendimientos. A continuaci√≥n se discutir√°n los aspectos relevantes de su propuesta:

La cualidad que me parece m√°s destacable del esquema es la idea de poder calcular similitudes con anterioridad y, a partir de estas, hacer **_prunning_** para disminuir la carga posterior del procesamiento. Y que, si bien en el paper se propone un m√©todo que termina con _top-k_ vecinos:

> For each item _j_ we compute the _k_ most similar items, where _k_ << _n_ and record these item numbers and their similarities with _j_

Existen otras t√©cnicas interesantes, como la que se muestra en [1], donde que se hace _prunning_ dejando _items_ que tengan un m√≠nimo de ```m``` evaluciones de usuarios en com√∫n.

Esta l√≥gica es la clave de **escalabilidad** presentada en el paper, pues al agregar nuevos usuarios se puede asumir que la similitud entre _items_ tiene variaciones peque√±as o incluso despreciables. Adem√°s, esta misma es capaz de atacar el problema de reducci√≥n de rendimiento  en los casos donde hay gran **vac√≠os en los datos** (ie: usuarios nuevos que no tienen muchos _ratings_), pues el recomendador se basar√° en los _items_ en los que s√≠ se han hecho evaluaciones, lo que en definitiva mejorar√° la predicci√≥n para estos usuarios.

### üíª Experimento:
El experimento que se realiza en el paper se basa en un _dataset_ de evaluaci√≥n de pel√≠culas con 43.000 usuarios y m√°s de 3.500 pel√≠culas, de los cuales se seleccion√≥ al azar para obtener exactamente ```100.000 ratings```. Estos datos definieron un **sparsity level** de 0.9369 el cu√°l **no se vari√≥** en ning√∫n experimento.

Para m√≠ este es el primer punto en contra en su desarrollo, pues creo que ser√≠a necesario hacer un an√°lisis de sensibilidad sobre esta variable (_sparsity level_), ya que tampoco hay una justificaci√≥n para esta elecci√≥n (como por ejemplo, que estos sean los valores observados en las aplicaciones comerciales). En vez de esto se elige variar la porci√≥n de datos en _training/testings sets_, lo cu√°l no parece tener un significado te√≥rico muy profundo.

A partir de este _dataset_ se elige la m√©trica **_MAE (Mean Absolute Error)_** para la comparaci√≥n de rendimientos de diferentes esquemas. Si bien esta es una buena m√©trica para determinar la **precisi√≥n de las predicciones de _ratings_**, un sistema recomendador deber√≠a evaluar algo m√°s que solo esta magnitud.\
De hecho, existe una amplia gama de m√©tricas para la evaluaci√≥n de sistemas recomendadores como **Diversity**, **Covarage**, **Serendepity** y **Novelty** definidas (de muy buena manera) en [3], las cuales (me parece) son fundamentales a la hora de hablar de qu√© tan bueno es un sistema recomendador en su totalidad. Por ejemplo, el problema m√°s t√≠pico de quedar solo con _MAE_, es que las predicciones son muy similares a las _items_ que el usuario ya ha visto (y que podr√≠an encontrar por si mismo), esto lleva a que las predicciones de nuestro sistemas sean algo "irrelevantes" para el usuario.

### üìï Conclusi√≥n (vs _user-based CF_):
Creo que si bien el m√©todo presentado en el paper es eficiente para el caso estudiado (considerando solo la m√©trica de error descrita), este resultado est√° fuertemente sujeto al _dataset_ utilizado y el c√≥mo este est√° conformado. En [2] (_User-Based vs Item-Based Recommendation) se explica como la conformaci√≥n la organizaci√≥n del _set_ de datos afecta de forma positiva/negativa a ambos m√©todos. Desde esto, se dice que es el _ratio_  ```usuarios/items``` el que determina cu√°l m√©todo ser√° mejor, en espec√≠fico se dice que:

> (...) a small number of high-confidence neighbors is by far preferable to a large number of neighbors for wich the similarity weights are not trustable.

Es desde este _approach_ que para _databases_ con un n√∫mero mayor de usuarios que de _items_ (como lo son las bases de datos comerciales), los _item-based methods_ resultan tener mejores resultados (la similitud entre _items_ es m√°s representativa al haber m√°s usuarios). 




## üñá Referencias:

1. Schafer, J. B., Frankowski, D., Herlocker, J., & Sen, S. (2007). Collaborative filtering recommender systems. In The adaptive web (pp. 291-324).

2. Ricci, F., Rockach L., Shapira B. (2010). A Comprehensive Survey of Neighborhood-Based Recommendation Methods. Recommender Systems Handbook (pp. 37-76).

3. Anna B. (2016). Recommender Systems - It's Not All About the Accuracy. Recuperado de [Medium](https://gab41.lab41.org/recommender-systems-its-not-all-about-the-accuracy-562c7dceeaff).
