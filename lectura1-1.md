# ðŸ“– Critica: Item-based collaborative filtering recommendation algorithms...

### ðŸ“šContexto:
Los algoritmos que utilizan _Collaborative Filtering (CF)_ son mÃ©todos de recomendaciÃ³n que se basan en buscar similitudes entres usuarios o _items_ y, a partir de estas, generar recomendaciones. Estos algoritmos se pueden dividir en _memory-based methods_ y _model-base methods_, entre los primeros se encuentran los _user-based_ e _item-based_ (segÃºn [2]) que serÃ¡n los principales puntos de discusiÃ³n en el artÃ­culo.

Si bien algoritmos que emplean _CF_ y, especÃ­ficamente, _user-based CF_ son los algoritmos mÃ¡s populares y utilizados en la prÃ¡ctica (en los aÃ±os del paper), la expansiÃ³n de las aplicaciones _web_ y los millones de nuevos datos que estas generan ha revelado problemas especÃ­ficos en estos cuando se habla de **escalabilidad** o la **distribuciÃ³n datos**.

### ðŸ§¾ Item-Based CF:
Ante estas problemÃ¡ticas, los autores proponen un esquema _item-based CF_ que ataca ambos problemas y que, segÃºn los experimentos realizados, resulta en un menor tiempo de computo y mejores rendimientos. A continuaciÃ³n se discutirÃ¡n los aspectos relevantes de su propuesta:

La cualidad que me parece destacable del esquema es la idea de poder calcular similitudes con anterioridad y, a partir de estas, hacer **_prunning_** para disminuir la carga posterior del procesamiento. Y que, si bien en el paper se propone un mÃ©todo que termina con _top-k_ vecinos:

> For each item _j_ we compute the _k_ most similar items, where _k_ << _n_ and record these item numbers and their similarities with _j_

Existen otras tÃ©cnicas interesantes, como la que se muestra en [1], donde que se hace _prunning_ dejando _items_ que tengan un mÃ­nimo de ```m``` evaluciones de usuarios en comÃºn.

Esta lÃ³gica es la clave de **escalabilidad** presentada en el paper, pues al agregar nuevos usuarios se puede asumir que la similitud entre _items_ tiene variaciones pequeÃ±as o incluso despreciables. AdemÃ¡s, esta misma es capaz de atacar el problema de reducciÃ³n de rendimiento  en los casos donde la **distribuciÃ³n de datos** es pobre (ie: usuarios nuevos que no tienen muchos _ratings_), pues el recomendador se basarÃ¡ en los _items_ en los que sÃ­ se han hecho evaluaciones, lo que en definitiva mejorarÃ¡ la predicciÃ³n para estos usuarios.

### ðŸ’» ExperimentaciÃ³n:
El experimento que se realiza en el paper se basa en un _dataset_ de evaluaciÃ³n de pelÃ­culas con 43.000 usuarios y mÃ¡s de 3.500 pelÃ­culas, de los cuales se seleccionÃ³ al azar para obtener exactamente ```100.000 ratings```. Estos datos definieron un **sparcity level** de 0.9369 el cuÃ¡l no se variÃ³ en ningÃºn experimento.

Para mÃ­ este es el primer punto en contra en sus experimentos, pues creo que serÃ­a necesario hacer un anÃ¡lisis de sensibilidad sobre esta variable (_sparcity level_), pues tampoco hay una justificaciÃ³n para esta elecciÃ³n (como por ejemplo, que estos sean los valores observados en las aplicaciones comerciales). En vez de esto se elige variar la porciÃ³n de datos en _training/testings sets_, lo cuÃ¡l no parece tener un significado teÃ³rico muy profundo.

A partir de este _dataset_ se elige la mÃ©trica **_MAE_** para la comparaciÃ³n de rendimientos de diferentes esquemas. Creo que si bien esta es una buena mÃ©trica para determinar la **precisiÃ³n de las predicciones**, en un sistema recomendador se deberÃ­a evaluar algo mÃ¡s que eso (no evalua la **calidad**). En mis lecturas me topÃ© con el tÃ©rmino **_serendipity_** que lo aplico a la idea de que este esquema no le ofrece al usuario una recomendaciÃ³n a la cual ella/Ã©l no pueda acceder (ie: haciendo una bÃºsqueda rÃ¡pida de un director que le guste), es decir, se podrÃ­a argumentar sobre si estÃ¡s predicciones son realmente valiosas para el usuario o si simplemente son predicciones "seguras" (yo creo que son las segunda).

### ðŸ“¥ Palabras Finales:
Creo que si bien el mÃ©todo presentado en el paper es eficiente para el caso estudiado (considerando la mÃ©trica _MAE_), esto estÃ¡ fuertemente sujeto al _dataset_ utilizado. En [2] se hace una referencia importante a que es la relaciÃ³n ```usuarios/items``` la que genera mayor impacto en cuanto al rendimiento de mÃ©todos _user-based_ vs _item_based_, siendo el mÃ©todo propuesto en el paper mejor en los casos de donde existen muchos mÃ¡s usuarios que productos.




## ðŸ–‡ Referencias:

1. Schafer, J. B., Frankowski, D., Herlocker, J., & Sen, S. (2007). Collaborative filtering recommender systems. In The adaptive web (pp. 291-324).

2. Ricci, F., Rockach L., Shapira B. (2010). A Comprehensive Survey of Neighborhood-Based Recommendation Methods. Recommender Systems Handbook (pp. 37-76).
